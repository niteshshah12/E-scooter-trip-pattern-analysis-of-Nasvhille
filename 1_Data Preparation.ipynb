{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:28:04.082891Z",
     "start_time": "2021-08-19T20:28:02.713495Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "#other library\n",
    "import ast\n",
    "from math import radians, cos, sin, asin, sqrt \n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "style.use('seaborn-darkgrid')\n",
    "\n",
    "# import folium\n",
    "\n",
    "import ast\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:28:04.098986Z",
     "start_time": "2021-08-19T20:28:04.083971Z"
    }
   },
   "outputs": [],
   "source": [
    "def high_trip_volume_days():\n",
    "    data = pd.read_csv('_Data/trip data/trip_dataset.csv')#, nrows=2000)  #.sample(5000)\n",
    "\n",
    "    #     #sample small data to test code BUT comment out for final script\n",
    "    #empty dictionary to track removed trips\n",
    "    data_count = {'Number of trips before slicing date: ': len(data)}\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    #CONVERT START AND END DATE AND TIME TO PANDAS DATETIME FORMAT\n",
    "    #drop row with null startDateTime and endDateTime\n",
    "    data = data.dropna(subset=['startDate', 'startTime', 'endDate', 'endTime'])\n",
    "    data['startDateTime'] = data.apply(lambda x: pd.to_datetime(\n",
    "        x['startDate'][:10] + ' ' + x['startTime'][:8],\n",
    "        format='%Y-%m-%d %H:%M:%S',\n",
    "        utc=True).tz_convert('US/Central'),\n",
    "                                       axis=1)\n",
    "    data['start_date_str'] = data.apply(\n",
    "        lambda x: str(x['startDateTime'].date()), axis=1)\n",
    "\n",
    "\n",
    "    # data_count = data.set_index('startDateTime')[['tripDuration'#,'route_directness_trace','tripDuration_m'#, 'distance_GPS_trace_miles'\n",
    "    # ]].groupby().resample('D').count()\n",
    "    return data['start_date_str'].value_counts(ascending=False)\n",
    "\n",
    "# high_trip_volume_days_sorted=high_trip_volume_days()\n",
    "# high_trip_volume_days_sorted[:15].index\n",
    "# high_trip_volume_days_sorted[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:28:04.114148Z",
     "start_time": "2021-08-19T20:28:04.100212Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_AssumedUTC_to_AssumeCentral(trip_Central):\n",
    "    '''\n",
    "    This function converts assumed UTC to assumed Central time zone\n",
    "    '''\n",
    "\n",
    "    #convert Assumed UTC and converted Central back to UTC\n",
    "    trip_starttime_AssumeUTC_InCentral = pd.to_datetime(trip_Central, utc=True)\n",
    "    #     print('Local (Assumed UTC in Central)', trip_starttime_AssumeUTC_InCentral)\n",
    "\n",
    "    trip_starttime_AssumeUTC_CorrectedCentral = trip_starttime_AssumeUTC_InCentral.tz_localize(\n",
    "        None).tz_localize('US/Central', nonexistent='NaT', ambiguous='NaT')\n",
    "    #     print('Local (Assumed UTC in Central) converted to Central',\n",
    "    #           trip_starttime_AssumeUTC_CorrectedCentral)\n",
    "\n",
    "    return trip_starttime_AssumeUTC_CorrectedCentral\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:28:04.130076Z",
     "start_time": "2021-08-19T20:28:04.118130Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('_Data/trip data/trip_dataset.csv').sample(10000)\n",
    "#     #                        ,nrows=2000)  # .sample(5000)\n",
    "\n",
    "#     #     #sample small data to test code BUT comment out for final script\n",
    "#     # empty dictionary to track removed trips\n",
    "# data_count = {'Number of trips before slicing date: ': len(data)}\n",
    "\n",
    "# # ---------------------------------------------------------------------------------------------------------------\n",
    "# # CONVERT START AND END DATE AND TIME TO PANDAS DATETIME FORMAT\n",
    "# # drop row with null startDateTime and endDateTime\n",
    "# data = data.dropna(subset=['startDate', 'startTime', 'endDate', 'endTime'])\n",
    "# data['startDateTime'] = data.apply(lambda x: pd.to_datetime(\n",
    "#     x['startDate'][:10] + ' ' + x['startTime'][:8],\n",
    "#     format='%Y-%m-%d %H:%M:%S',\n",
    "#     utc=True).tz_convert('US/Central'),\n",
    "#                                    axis=1)\n",
    "# data['endDateTime'] = data.apply(\n",
    "#     lambda x: pd.to_datetime(x['endDate'][:10] + ' ' + x['endTime'][:8],\n",
    "#                              format='%Y-%m-%d %H:%M:%S',\n",
    "#                              utc=True).tz_convert('US/Central'),\n",
    "#     axis=1)\n",
    "\n",
    "# #data['pubTimeStamp_pd'] = data.apply(lambda x: pd.to_datetimetime(x['pubTimeStamp'], format='%Y-%m-%d %H:%M:%S'), axis=1)\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------------\n",
    "# # SLICE DATA FROM 2018-9-01 to 2019-8-31\n",
    "# data = data.loc[(data['startDateTime'] > '2018-9-01')\n",
    "#                 & (data['startDateTime'] < '2019-8-31')]\n",
    "# data_count.update({'Total number of trips for analysis period': len(data)})\n",
    "\n",
    "# #correct lime data\n",
    "# data['startDateTime_month'] = data.apply(\n",
    "#     lambda x: x['startDateTime'].strftime('%Y-%m'), axis=1)\n",
    "\n",
    "# data['startDateTime_corrected'] = data.apply(\n",
    "#     lambda x: convert_AssumedUTC_to_AssumeCentral(x['startDateTime'])\n",
    "#     if (x['startDateTime_month'] in [\n",
    "#         '2019-01', '2019-02', '2019-03', '2019-04'\n",
    "#     ]) and x['companyName'] == 'Lime' else x['startDateTime'],\n",
    "#     axis=1)\n",
    "\n",
    "# data['endDateTime_corrected'] = data.apply(\n",
    "#     lambda x: convert_AssumedUTC_to_AssumeCentral(x['endDateTime'])\n",
    "#     if (x['startDateTime_month'] in [\n",
    "#         '2019-01', '2019-02', '2019-03', '2019-04'\n",
    "#     ]) and x['companyName'] == 'Lime' else x['endDateTime'],\n",
    "#     axis=1)\n",
    "# data.head()\n",
    "# data[data['companyName']=='Lime'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:24:34.339660Z",
     "start_time": "2021-08-19T20:24:34.295457Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:28:04.176865Z",
     "start_time": "2021-08-19T20:28:04.131130Z"
    }
   },
   "outputs": [],
   "source": [
    "'''This function reads the raw data and cleans the data based on basic criteria. '''\n",
    "\n",
    "\n",
    "def basic_cleaning():\n",
    "    # read raw data file\n",
    "    data = pd.read_csv('_Data/trip data/trip_dataset.csv')#.sample(10000)\n",
    "    #                        ,nrows=2000)  # .sample(5000)\n",
    "\n",
    "    #     #sample small data to test code BUT comment out for final script\n",
    "    # empty dictionary to track removed trips\n",
    "    data_count = {'Number of trips before slicing date: ': len(data)}\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------\n",
    "    # CONVERT START AND END DATE AND TIME TO PANDAS DATETIME FORMAT\n",
    "    # drop row with null startDateTime and endDateTime\n",
    "    data = data.dropna(subset=['startDate', 'startTime', 'endDate', 'endTime'])\n",
    "    data['startDateTime'] = data.apply(lambda x: pd.to_datetime(\n",
    "        x['startDate'][:10] + ' ' + x['startTime'][:8],\n",
    "        format='%Y-%m-%d %H:%M:%S',\n",
    "        utc=True).tz_convert('US/Central'),\n",
    "                                       axis=1)\n",
    "    data['endDateTime'] = data.apply(\n",
    "        lambda x: pd.to_datetime(x['endDate'][:10] + ' ' + x['endTime'][:8],\n",
    "                                 format='%Y-%m-%d %H:%M:%S',\n",
    "                                 utc=True).tz_convert('US/Central'),\n",
    "        axis=1)\n",
    "\n",
    "    #data['pubTimeStamp_pd'] = data.apply(lambda x: pd.to_datetimetime(x['pubTimeStamp'], format='%Y-%m-%d %H:%M:%S'), axis=1)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # SLICE DATA FROM 2018-9-01 to 2019-8-31\n",
    "    data = data.loc[(data['startDateTime'] > '2018-9-01')\n",
    "                    & (data['startDateTime'] < '2019-8-31')]\n",
    "    data_count.update({'Total number of trips for analysis period': len(data)})\n",
    "\n",
    "    #correct lime data\n",
    "    data['startDateTime_month'] = data.apply(\n",
    "        lambda x: x['startDateTime'].strftime('%Y-%m'), axis=1)\n",
    "\n",
    "    data['startDateTime'] = data.apply(\n",
    "        lambda x: convert_AssumedUTC_to_AssumeCentral(x['startDateTime'])\n",
    "        if (x['startDateTime_month'] in [\n",
    "            '2019-01', '2019-02', '2019-03', '2019-04'\n",
    "        ]) and x['companyName'] == 'Lime' else x['startDateTime'],\n",
    "        axis=1)\n",
    "\n",
    "    data['endDateTime'] = data.apply(\n",
    "        lambda x: convert_AssumedUTC_to_AssumeCentral(x['endDateTime'])\n",
    "        if (x['startDateTime_month'] in [\n",
    "            '2019-01', '2019-02', '2019-03', '2019-04'\n",
    "        ]) and x['companyName'] == 'Lime' else x['endDateTime'],\n",
    "        axis=1)\n",
    "\n",
    "    initial_trip_len = len(data)\n",
    "\n",
    "    # rename index column as trip ID\n",
    "    data = data.reset_index().rename(columns={'index': 'trip_id'})\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # PRELIMINARY DATA CLEANING\n",
    "\n",
    "    # REMOVE MISSING VALUES\n",
    "    # drop na variables and store remaining number of observation\n",
    "    len_before = len(data)\n",
    "    data = data.dropna()\n",
    "    data_count.update({'Trips with Nan': len_before - len(data)})\n",
    "\n",
    "    # remove trips whose trip duration is between 1 and 180 minutes inclusive\n",
    "    len_before = len(data)\n",
    "    data = data[data['tripDuration'].between(1, 180)]\n",
    "    data_count.update({\n",
    "        'Trips with duration other than [1:180] minute':\n",
    "        len_before - len(data)\n",
    "    })\n",
    "\n",
    "    # remove trips whose trip distance is between 1 and 10 miles inclusive\n",
    "    len_before = len(data)\n",
    "    data = data[data['tripDistance'].between(1, 10 * 5280)]\n",
    "    data_count.update({\n",
    "        'Trips with duration other than [0 :10 miles]':\n",
    "        len_before - len(data)\n",
    "    })\n",
    "\n",
    "    # remove observation with too few GPS trace data\n",
    "    len_before = len(data)\n",
    "    data = data[data['tripRoute'].str.len() > 40]\n",
    "    data_count.update({'Trips with few GPS points': len_before - len(data)})\n",
    "\n",
    "    # DATA CLEANING BECAUSE OF SOME GLITCH\n",
    "    # remove special character (\"\") from the data\n",
    "    data['tripRoute'] = data.apply(\n",
    "        lambda x: x['tripRoute'].strip().replace('\"', ''), axis=1)\n",
    "\n",
    "    # REMOVING REDUNDANT COLUMNS AND TRANSFORMING COLUMNS\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # calcute distance between two WGS points and returns value in ft\n",
    "    def distance(lat1, lon1, lat2, lon2):\n",
    "        # https://www.geeksforgeeks.org/program-distance-two-points-earth/\n",
    "\n",
    "        # The math module contains a function named\n",
    "        # radians which converts from degrees to radians.\n",
    "        lon1 = radians(lon1)\n",
    "        lon2 = radians(lon2)\n",
    "        lat1 = radians(lat1)\n",
    "        lat2 = radians(lat2)\n",
    "\n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "\n",
    "        # Radius of earth in kilometers. Use 3956 for miles\n",
    "        r = 3956\n",
    "\n",
    "        # calculate the result\n",
    "        return (c * r * 5280)\n",
    "\n",
    "    def trip_distance(GPS_trace_data):\n",
    "        try:\n",
    "            a = 0\n",
    "            # remove empty list in tripRoute data\n",
    "            GPS_trace_data = [x for x in GPS_trace_data if x != []]\n",
    "\n",
    "            for point_id in np.arange(len(GPS_trace_data) - 1):\n",
    "                a += distance(GPS_trace_data[point_id][0],\n",
    "                              GPS_trace_data[point_id][1],\n",
    "                              GPS_trace_data[point_id + 1][0],\n",
    "                              GPS_trace_data[point_id + 1][1])\n",
    "            return a\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # function to swap lat and long\n",
    "    def swap(x):\n",
    "        coords = list(x.coords)\n",
    "        coords = [\n",
    "            Point(t[1], t[0]) for t in coords\n",
    "        ]  # Swap each coordinate using list comprehension and create Points\n",
    "        return LineString(coords)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "\n",
    "    # drop columns that are redundant\n",
    "    data = data.drop(columns=[\n",
    "        'startDate', 'startTime', 'endDate', 'endTime', 'create_dt',\n",
    "        'pubTimeStamp'\n",
    "    ])\n",
    "    data['tripDistance_miles'] = data.apply(lambda x: x['tripDistance'] / 5280,\n",
    "                                            axis=1)\n",
    "\n",
    "    data['euclidean_distance'] = data.apply(\n",
    "        lambda x: distance(x['startLatitude'], x['startLongitude'], x[\n",
    "            'endLatitude'], x['endLongitude']),\n",
    "        axis=1)\n",
    "\n",
    "    data['distance_GPS_trace'] = data.apply(\n",
    "        lambda x: trip_distance(ast.literal_eval(x['tripRoute'])), axis=1)\n",
    "\n",
    "    data['distance_GPS_trace_miles'] = data.apply(\n",
    "        lambda x: x['distance_GPS_trace'] / 5280, axis=1)\n",
    "\n",
    "    # dummy variable for trip start time\n",
    "    data['trip_start_hour_no'] = data.apply(\n",
    "        lambda x: x['startDateTime'].hour + 1, axis=1)\n",
    "\n",
    "    data['StartTime_AMPeak'] = data.apply(lambda x: 1 if x[\n",
    "        'trip_start_hour_no'] > 7 and x['trip_start_hour_no'] <= 10 else 0,\n",
    "                                          axis=1)\n",
    "    data['StartTime_Day'] = data.apply(lambda x: 1 if x[\n",
    "        'trip_start_hour_no'] > 10 and x['trip_start_hour_no'] <= 16 else 0,\n",
    "                                       axis=1)\n",
    "    data['StartTime_PMPeak'] = data.apply(lambda x: 1 if x[\n",
    "        'trip_start_hour_no'] > 16 and x['trip_start_hour_no'] <= 20 else 0,\n",
    "                                          axis=1)\n",
    "    data['StartTime_Night'] = data.apply(\n",
    "        lambda x: 1\n",
    "        if x['trip_start_hour_no'] > 20 or x['trip_start_hour_no'] <= 7 else 0,\n",
    "        axis=1)\n",
    "\n",
    "    data['StartTime_decimal'] = data.apply(\n",
    "        lambda x: x['startDateTime'].hour + x['startDateTime'].minute / 60,\n",
    "        axis=1)\n",
    "\n",
    "    len_before = len(data)\n",
    "    data = data[data['euclidean_distance'] > 0]\n",
    "\n",
    "    # -----------------------------\n",
    "    data_count.update({'Same start and end points': len_before - len(data)})\n",
    "\n",
    "    #     len_before = len(data)\n",
    "    data = data[data['distance_GPS_trace'] > 0]\n",
    "    data_count.update({'zero GPS trace distance': len_before - len(data)})\n",
    "\n",
    "    # calculate trip directness\n",
    "    data['route_directness_trace'] = data.apply(\n",
    "        lambda x: x['euclidean_distance'] / x['distance_GPS_trace'], axis=1)\n",
    "    # -----------------------------\n",
    "\n",
    "    data['route_directness_rawDist'] = data.apply(\n",
    "        lambda x: x['euclidean_distance'] / x['tripDistance'], axis=1)\n",
    "    # -----------------------------\n",
    "    len_before = len(data)\n",
    "    data = data[data['route_directness_trace'] < 1]\n",
    "    data_count.update({'route directness > 1': len_before - len(data)})\n",
    "    # -----------------------------\n",
    "    data['average_trip_speed_mph'] = data.apply(\n",
    "        lambda x: x['tripDistance'] / (x['tripDuration'] * 60) * 0.681818,\n",
    "        axis=1)\n",
    "\n",
    "    # create a variable name with trip start hour and week number (Monday=0 and Sunday=6)\n",
    "    data['trip_start_week_no'] = data.apply(\n",
    "        lambda x: x['startDateTime'].weekday(), axis=1)\n",
    "\n",
    "    def flag_weekend_trip(trip_start_week_no, trip_start_hour_no):\n",
    "        weekend = 0\n",
    "        if trip_start_week_no >= 6:\n",
    "            weekend = 1\n",
    "        elif (trip_start_week_no == 5) & (trip_start_hour_no > 16):\n",
    "            weekend = 1\n",
    "\n",
    "        return weekend\n",
    "\n",
    "    data['weekend_trip'] = data.apply(lambda x: flag_weekend_trip(\n",
    "        x['trip_start_week_no'], x['trip_start_hour_no']),\n",
    "                                      axis=1)\n",
    "\n",
    "    # trip in high trip count day\n",
    "    high_trip_day = [\n",
    "        '2019-04-27', '2019-04-26', '2019-05-25', '2019-05-26', '2019-06-01',\n",
    "        '2019-04-25', '2019-06-08', '2019-05-18', '2019-04-28', '2019-06-15',\n",
    "        '2019-04-21', '2019-04-13', '2019-03-23', '2019-04-06', '2019-05-17'\n",
    "    ]\n",
    "\n",
    "    data['start_date_str'] = data.apply(\n",
    "        lambda x: str(x['startDateTime'].date()), axis=1)\n",
    "\n",
    "    data['high_trip_count_day'] = data.apply(\n",
    "        lambda x: 1 if x['start_date_str'] in high_trip_day else 0, axis=1)\n",
    "\n",
    "    data_count.update({'Total number of trips after cleaning': len(data)})\n",
    "    data_count.update(\n",
    "        {'Total number of trips removed': initial_trip_len - len(data)})\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # save data with all column\n",
    "    data.to_csv(\n",
    "        '_Data/trip data/cleaned_trip_data_withRouteData_TimeZoneCorrected_08192021.csv'\n",
    "    )\n",
    "    # save data without route data column\n",
    "    data.drop(columns=['tripRoute']).to_csv(\n",
    "        '_Data/trip data/cleaned_trip_data_withoutRouteData_TimeZoneCorrected_08192021.csv'\n",
    "    )\n",
    "    # data.sample(200000, random_state=0).to_csv(\n",
    "    #     '../1_Data/trip data/cleaned_sampled_trip_data_withRouteData_sample.csv'\n",
    "    # )\n",
    "\n",
    "    # export start and end points with trip_id\n",
    "    # data[['trip_id', 'startLatitude', 'startLongitude',\n",
    "    #       'weekend_flag']].to_csv('../1_Data/trip data/start_points.csv')\n",
    "    # data[['trip_id', 'endLatitude', 'endLongitude',\n",
    "    #       'weekend_flag']].to_csv('../1_Data/trip data/end_points.csv')\n",
    "\n",
    "    data[[\n",
    "        'trip_id', 'startLatitude', 'startLongitude', 'endLatitude',\n",
    "        'endLongitude', 'weekend_trip', 'startDateTime', 'tripRoute'\n",
    "    ]].to_csv(\n",
    "        '_Data/trip data/start_end_points_cleaned_TimeZoneCorrected_08192021.csv'\n",
    "    )\n",
    "\n",
    "    return data, data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.631Z"
    }
   },
   "outputs": [],
   "source": [
    "#call function to read and clean the data\n",
    "(trip_data, count) = basic_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore corrected timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.635Z"
    }
   },
   "outputs": [],
   "source": [
    "trip_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.638Z"
    }
   },
   "outputs": [],
   "source": [
    "#loop over each operator to plot starttime of the day\n",
    "for operator in trip_data['companyName'].unique():\n",
    "    style.use('seaborn-darkgrid')\n",
    "    plot, ax = plt.subplots()\n",
    "    plot = sns.kdeplot(data=trip_data[trip_data['companyName'] == operator],\n",
    "                       x='trip_start_hour_no',\n",
    "                       hue='startDateTime_month')\n",
    "\n",
    "    sns.set(rc={'figure.figsize': (11, 5)})\n",
    "    plot.set(title=operator)\n",
    "    plot.set(xlim=(0, 24))\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig(\n",
    "        str('_Data/trip data/time check/' + operator +\n",
    "            '_kde_afterCorrecting.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.640Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"lenght of raw data: \",len(trip_data))\n",
    "print('Cleaning')\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.642Z"
    }
   },
   "outputs": [],
   "source": [
    "print('percentage of trips removed: ', 474490/1546920*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.645Z"
    }
   },
   "outputs": [],
   "source": [
    "trip_data[['startDateTime', 'endDateTime',]].head() #verifyied that datetime is corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link points to GRID_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.647Z"
    }
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "grid_gdf = gpd.read_file(\"_Data/Landuse data/Shapefiles/Fishnet_250m_selectedGrids.shp\")\n",
    "trip_df=pd.read_csv(\"_Data/trip data/start_end_points_cleaned_TimeZoneCorrected_08192021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.649Z"
    }
   },
   "outputs": [],
   "source": [
    "def link_startpoint_grid(points_df):\n",
    "    # creating a geometry column\n",
    "    geometry_start = [\n",
    "        Point(xy)\n",
    "        for xy in zip(points_df['startLongitude'], points_df['startLatitude'])\n",
    "    ]\n",
    "\n",
    "    # Coordinate reference system : WGS84\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "\n",
    "    # Creating a Geographic data frame\n",
    "    startpoints_gdf = gpd.GeoDataFrame(points_df,\n",
    "                                       crs=crs,\n",
    "                                       geometry=geometry_start)\n",
    "\n",
    "    #join points\n",
    "\n",
    "    startpoints_merged = gpd.sjoin(startpoints_gdf,\n",
    "                                   grid_gdf,\n",
    "                                   how=\"left\",\n",
    "                                   op='intersects')\n",
    "    startpoints_merged = startpoints_merged[['trip_id', 'GRID_ID']]\n",
    "\n",
    "    return pd.DataFrame(startpoints_merged).rename(columns={'GRID_ID':'GRID_ID_start'})\n",
    "\n",
    "def link_endpoint_grid(points_df):\n",
    "    # creating a geometry column\n",
    "    geometry_end = [\n",
    "        Point(xy)\n",
    "        for xy in zip(points_df['endLongitude'], points_df['endLatitude'])\n",
    "    ]\n",
    "\n",
    "    # Coordinate reference system : WGS84\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "\n",
    "    # Creating a Geographic data frame\n",
    "    endpoints_gdf = gpd.GeoDataFrame(points_df, crs=crs, geometry=geometry_end)\n",
    "\n",
    "    #join ebd points\n",
    "    endpoints_merged = gpd.sjoin(endpoints_gdf,\n",
    "                                 grid_gdf,\n",
    "                                 how=\"left\",\n",
    "                                 op='intersects')\n",
    "    endpoints_merged = endpoints_merged[['trip_id', 'GRID_ID']]\n",
    "\n",
    "    return pd.DataFrame(endpoints_merged).rename(columns={'GRID_ID':'GRID_ID_end'})\n",
    "\n",
    "\n",
    "#call function\n",
    "startpoint_grids = link_startpoint_grid(trip_df[['trip_id', 'startLatitude', 'startLongitude']])\n",
    "endpoint_grids = link_endpoint_grid(trip_df[['trip_id', 'endLatitude', 'endLongitude']])\n",
    "\n",
    "#create a dataframe that links trip_id with start and end points\n",
    "startend_grid= pd.merge(startpoint_grids,endpoint_grids, on='trip_id', how='inner').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.650Z"
    }
   },
   "outputs": [],
   "source": [
    "count.update({'Trips dropped for missing value in GRID': len(trip_data)-len(startend_grid)})\n",
    "print('Number of trips dropped: ',len(trip_data)-len(startend_grid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link grid level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.652Z"
    }
   },
   "outputs": [],
   "source": [
    "landuse_byGrid = pd.DataFrame(\n",
    "    gpd.read_file(\n",
    "        '_Data/Landuse data/results/Shapefile/LanduseData_ByGrid.shp')\n",
    ").drop(\n",
    "    columns=['OBJECTID', 'Shape_Leng', 'Shape_Area', 'Sum_BuiltE', 'geometry'\n",
    "             ]).rename(columns={'Fishnet_25': 'GRID_ID'})\n",
    "#average short and long term parking\n",
    "landuse_byGrid['MEAN_PAKING'] = landuse_byGrid[['MEAN_SHT_P','MEAN_LNG_P']].mean(axis=1)\n",
    "landuse_byGrid=landuse_byGrid.drop(columns=['MEAN_SHT_P','MEAN_LNG_P'])\n",
    "\n",
    "enthropy_byGrid = pd.read_csv(\n",
    "    '_Data/Landuse data/results/enthropy_value.csv')[[\n",
    "        'Fishnet_25', 'enthropy'\n",
    "    ]].rename(columns={'Fishnet_25': 'GRID_ID'})\n",
    "\n",
    "intersection_density_byGrid = pd.read_csv(\n",
    "    '_Data/Landuse data/results/intersection_density.csv')[[\n",
    "        'GRID_ID', 'intersection_density'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.654Z"
    }
   },
   "outputs": [],
   "source": [
    "#Merge by joining columns to startend_grid dataframe \n",
    "#start points\n",
    "#enthropy\n",
    "landuse_merged = pd.merge(startend_grid,\n",
    "                          enthropy_byGrid.add_suffix('_start'),\n",
    "                          left_on='GRID_ID_start',\n",
    "                          right_on='GRID_ID_start',\n",
    "                          how='left')  \n",
    "\n",
    "#landuse\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          landuse_byGrid.add_suffix('_start'),\n",
    "                          left_on='GRID_ID_start',\n",
    "                          right_on='GRID_ID_start',\n",
    "                          how='left') \n",
    "#intersection density\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          intersection_density_byGrid.add_suffix('_start'),\n",
    "                          left_on='GRID_ID_start',\n",
    "                          right_on='GRID_ID_start',\n",
    "                          how='left') \n",
    "\n",
    "##End points\n",
    "#enthropy\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          enthropy_byGrid.add_suffix('_end'),\n",
    "                          left_on='GRID_ID_end',\n",
    "                          right_on='GRID_ID_end',\n",
    "                          how='left')  \n",
    "\n",
    "#landuse\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          landuse_byGrid.add_suffix('_end'),\n",
    "                          left_on='GRID_ID_end',\n",
    "                          right_on='GRID_ID_end',\n",
    "                          how='left') \n",
    "#intersection density\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          intersection_density_byGrid.add_suffix('_end'),\n",
    "                          left_on='GRID_ID_end',\n",
    "                          right_on='GRID_ID_end',\n",
    "                          how='left') \n",
    "\n",
    "print(landuse_merged.shape)\n",
    "landuse_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link trip level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T02:56:52.128370Z",
     "start_time": "2020-05-12T02:56:52.119365Z"
    }
   },
   "source": [
    "### Flag locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.656Z"
    }
   },
   "outputs": [],
   "source": [
    "park_shp=gpd.read_file(\n",
    "        '_Data/Landuse data/results/Shapefile/Park.shp')\n",
    "vanderbilt_shp=gpd.read_file(\n",
    "        '_Data/Landuse data/results/Shapefile/Vanderbilt_University.shp')\n",
    "nissan_shp=gpd.read_file(\n",
    "        '_Data/Landuse data/results/Shapefile/Nissan_stadium.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.658Z"
    }
   },
   "outputs": [],
   "source": [
    "def link_startpoint_flag(points_df, flag_shp,col_name):\n",
    "    # creating a geometry column\n",
    "    geometry_start = [\n",
    "        Point(xy)\n",
    "        for xy in zip(points_df['startLongitude'], points_df['startLatitude'])\n",
    "    ]\n",
    "\n",
    "    # Coordinate reference system : WGS84\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "\n",
    "    # Creating a Geographic data frame\n",
    "    startpoints_gdf = gpd.GeoDataFrame(points_df,\n",
    "                                       crs=crs,\n",
    "                                       geometry=geometry_start)\n",
    "\n",
    "    #join points\n",
    "\n",
    "    startpoints_merged = gpd.sjoin(startpoints_gdf,\n",
    "                                   flag_shp,\n",
    "                                   how=\"left\",\n",
    "                                   op='intersects')\n",
    "    startpoints_merged = startpoints_merged[['trip_id', col_name]]\n",
    "    start_point_pd = pd.DataFrame(startpoints_merged)\n",
    "    start_point_pd[col_name]=start_point_pd[col_name].fillna(0)\n",
    "    \n",
    "    return start_point_pd\n",
    "\n",
    "link_park_start=link_startpoint_flag(trip_df[['trip_id', 'startLatitude', 'startLongitude']], park_shp,'PARK')\n",
    "\n",
    "link_vanderbilt_start = link_startpoint_flag(\n",
    "    trip_df[['trip_id', 'startLatitude', 'startLongitude']], vanderbilt_shp, 'VANDERBILT')\n",
    "\n",
    "link_nissan_start = link_startpoint_flag(\n",
    "    trip_df[['trip_id', 'startLatitude', 'startLongitude']], nissan_shp,\n",
    "    'NFL_DRAFT').rename(columns={'NFL_DRAFT': 'NISSAN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.659Z"
    }
   },
   "outputs": [],
   "source": [
    "#link trip data for starts\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          link_park_start.add_suffix('_start'),\n",
    "                          left_on='trip_id',\n",
    "                          right_on='trip_id_start',\n",
    "                          how='left').drop(columns=['trip_id_start'])\n",
    "\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          link_vanderbilt_start.add_suffix('_start'),\n",
    "                          left_on='trip_id',\n",
    "                          right_on='trip_id_start',\n",
    "                          how='left').drop(columns=['trip_id_start'])\n",
    "\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          link_nissan_start.add_suffix('_start'),\n",
    "                          left_on='trip_id',\n",
    "                          right_on='trip_id_start',\n",
    "                          how='left').drop(columns=['trip_id_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.661Z"
    }
   },
   "outputs": [],
   "source": [
    "def link_endpoint_grid(points_df, flag_shp, col_name):\n",
    "    # creating a geometry column\n",
    "    geometry_end = [\n",
    "        Point(xy)\n",
    "        for xy in zip(points_df['endLongitude'], points_df['endLatitude'])\n",
    "    ]\n",
    "\n",
    "    # Coordinate reference system : WGS84\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "\n",
    "    # Creating a Geographic data frame\n",
    "    endpoints_gdf = gpd.GeoDataFrame(points_df, crs=crs, geometry=geometry_end)\n",
    "\n",
    "    #join points\n",
    "\n",
    "    endpoints_merged = gpd.sjoin(endpoints_gdf,\n",
    "                                 flag_shp,\n",
    "                                 how=\"left\",\n",
    "                                 op='intersects')\n",
    "    endpoints_merged = endpoints_merged[['trip_id', col_name]]\n",
    "    end_point_pd = pd.DataFrame(endpoints_merged)\n",
    "    end_point_pd[col_name] = end_point_pd[col_name].fillna(0)\n",
    "\n",
    "    return end_point_pd\n",
    "\n",
    "\n",
    "link_park_end = link_endpoint_grid(\n",
    "    trip_df[['trip_id', 'endLatitude', 'endLongitude']], park_shp, 'PARK')\n",
    "\n",
    "link_vanderbilt_end = link_endpoint_grid(\n",
    "    trip_df[['trip_id', 'endLatitude', 'endLongitude']], vanderbilt_shp,\n",
    "    'VANDERBILT')\n",
    "\n",
    "link_nissan_end = link_endpoint_grid(\n",
    "    trip_df[['trip_id', 'endLatitude', 'endLongitude']], nissan_shp,\n",
    "    'NFL_DRAFT').rename(columns={'NFL_DRAFT': 'NISSAN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.663Z"
    }
   },
   "outputs": [],
   "source": [
    "#link trip data for ends\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          link_park_end.add_suffix('_end'),\n",
    "                          left_on='trip_id',\n",
    "                          right_on='trip_id_end',\n",
    "                          how='left').drop(columns=['trip_id_end'])\n",
    "\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          link_vanderbilt_end.add_suffix('_end'),\n",
    "                          left_on='trip_id',\n",
    "                          right_on='trip_id_end',\n",
    "                          how='left').drop(columns=['trip_id_end'])\n",
    "\n",
    "landuse_merged = pd.merge(landuse_merged,\n",
    "                          link_nissan_end.add_suffix('_end'),\n",
    "                          left_on='trip_id',\n",
    "                          right_on='trip_id_end',\n",
    "                          how='left').drop(columns=['trip_id_end'])\n",
    "print(landuse_merged.shape)\n",
    "landuse_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.664Z"
    }
   },
   "outputs": [],
   "source": [
    "weather_data = pd.read_excel(\n",
    "    io='_Data/weather data/NashvilleAirport_data.xlsx')\n",
    "\n",
    "# trip_data['start_date_str'] = trip_data.apply(\n",
    "#     lambda x: str(x['startDateTime'].date()), axis=1)\n",
    "weather_data['DATE_str'] = weather_data.apply(lambda x: str(x['DATE'].date()),\n",
    "                                              axis=1)\n",
    "\n",
    "weather_data=weather_data[['DATE_str','PRCP','TAVG']]\n",
    "weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.666Z"
    }
   },
   "outputs": [],
   "source": [
    "#column names in trip data\n",
    "trip_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.668Z"
    }
   },
   "outputs": [],
   "source": [
    "#select columns\n",
    "trip_data_selected = trip_data[[\n",
    "    'trip_id', 'tripDistance_miles', 'tripDistance', 'tripDuration', 'euclidean_distance', 'StartTime_decimal',\n",
    "    'route_directness_rawDist','route_directness_trace', 'average_trip_speed_mph', 'trip_start_week_no',\n",
    "    'trip_start_hour_no', 'StartTime_AMPeak', 'StartTime_Day',\n",
    "    'StartTime_PMPeak', 'StartTime_Night', 'weekend_trip', 'start_date_str',\n",
    "    'high_trip_count_day'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.669Z"
    }
   },
   "outputs": [],
   "source": [
    "#merge lanuse and trip data\n",
    "landuse_trip_df=pd.merge(landuse_merged,\n",
    "         trip_data_selected,\n",
    "         left_on='trip_id',\n",
    "         right_on='trip_id',\n",
    "         how='left')\n",
    "print(landuse_trip_df.shape)\n",
    "landuse_trip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.671Z"
    }
   },
   "outputs": [],
   "source": [
    "#merge weather data\n",
    "landuse_trip_weather_df=pd.merge(landuse_trip_df,\n",
    "         weather_data,\n",
    "         left_on='start_date_str',\n",
    "         right_on='DATE_str',\n",
    "         how='left').drop(columns=['start_date_str'])\n",
    "print(landuse_trip_weather_df.shape)\n",
    "landuse_trip_weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.673Z"
    }
   },
   "outputs": [],
   "source": [
    "#missing data\n",
    "landuse_trip_weather_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:26:53.578600Z",
     "start_time": "2020-05-12T19:26:53.572616Z"
    }
   },
   "source": [
    "The corrosponding GRIDs of na values of enthropy have no POI. The enthropy of such grid is zero. Therefore, we can replace missing values with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.674Z"
    }
   },
   "outputs": [],
   "source": [
    "landuse_trip_weather_df=landuse_trip_weather_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.676Z"
    }
   },
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.678Z"
    }
   },
   "outputs": [],
   "source": [
    "print('percentage of trips removed for missing GRID', 22389/1546920*100)\n",
    "print('number of trips removed overall', 1546920-1050041)\n",
    "print('percentage of trips removed overall', 100-1050041/1546920*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.680Z"
    }
   },
   "outputs": [],
   "source": [
    "landuse_trip_weather_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-19T20:28:02.681Z"
    }
   },
   "outputs": [],
   "source": [
    "landuse_trip_weather_df.to_csv(\n",
    "    '_Data/model data/model_data_TimeZoneCorrected_08192021.csv')\n",
    "# landuse_trip_weather_df.sample(\n",
    "#     5000, random_state=0).to_csv('_Data/model data/model_data_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "789px",
    "left": "1536px",
    "top": "116px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 250.79700000000003,
   "position": {
    "height": "272.795px",
    "left": "1607.99px",
    "right": "20px",
    "top": "593.99px",
    "width": "363.16px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
